#!/bin/sh
#SBATCH --partition=gpuextralong
#SBATCH --time=504:00:00
#SBATCH --gres=gpu:4
#SBATCH --mem-per-gpu=32000
#SBATCH --job-name=NER
#SBATCH --err=NER.err 
#SBATCH --out=NER.out 
#SBATCH --mail-user=kangkangsome@gmail.com    # where send info about job
#SBATCH --mail-type=ALL              # what to send, valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL

/bin/hostname
srun -l /bin/hostname
srun -l /bin/pwd
ml load Python/3.6.8-foss-2019a
cd /home/kangchen/Rehrearsal_TransferLearning/python_script/BERT-GLUE/
source glueEnv/bin/activate
python transformers/examples/pytorch/token-classification/run_ner.py --model_name_or_path bert-large-cased --dataset_name conll2003 --do_train --do_eval --num_train_epochs 20 --overwrite_output_dir --output_dir output/NER/BERT/
python transformers/examples/pytorch/token-classification/run_ner.py --model_name_or_path albert-large-v2 --dataset_name conll2003 --do_train --do_eval --num_train_epochs 20 --overwrite_output_dir --output_dir output/NER/ALBERTRT/
python transformers/examples/pytorch/token-classification/run_ner.py --model_name_or_path roberta-large --dataset_name conll2003 --do_train --do_eval --num_train_epochs 20 --overwrite_output_dir --output_dir output/NER/RoBERTa/
python transformers/examples/pytorch/token-classification/run_ner.py --model_name_or_path squeezebert/squeezebert-uncased --dataset_name conll2003 --do_train --do_eval --num_train_epochs 20 --overwrite_output_dir --output_dir output/NER/SqueezeBERT/
python transformers/examples/pytorch/token-classification/run_ner.py --model_name_or_path microsoft/deberta-v3-large --dataset_name conll2003 --do_train --do_eval --num_train_epochs 20 --overwrite_output_dir --output_dir output/NER/DeBERTa/