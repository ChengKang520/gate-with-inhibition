
# giMLP in Transformers and CNNs

giMLPs: Gate with Inhibition Mechanism in MLPs


## Introduction

RoBERTa iterates on BERT's pretraining procedure, including training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. See the associated paper for more details.

### What's New:

- (05/08/2022) Added [giMLPs On ImageNet Classification](/giMLP_CNN/README.md) and [giMLP on Transformers On Downstream Language Tasks Fine-Tuning 
](/giMLP_Transformers/README.md).
- (04/08/2022) Initial release.
